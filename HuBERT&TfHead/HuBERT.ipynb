{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from transformers import HubertModel, AutoConfig\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['anger', 'fear', 'happy', 'neutral', 'sad']\n",
    "label_to_id = {label: i for i, label in enumerate(emotion_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(emotion_labels)}\n",
    "\n",
    "# Audio Processing Configuration\n",
    "MAX_AUDIO_LENGTH = 160000  # 10 seconds at 16kHz\n",
    "TARGET_SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching mechanism for processed audio\n",
    "class AudioCache:\n",
    "    def __init__(self, cache_size=1000):\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "    def get(self, path):\n",
    "        if path in self.cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.cache[path]\n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, path, data):\n",
    "        # Simple LRU-like behavior - clear cache if it gets too big\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            # Keep only 75% of the cache (remove oldest entries)\n",
    "            keys_to_keep = list(self.cache.keys())[-int(self.cache_size * 0.75):]\n",
    "            new_cache = {k: self.cache[k] for k in keys_to_keep}\n",
    "            self.cache = new_cache\n",
    "        self.cache[path] = data\n",
    "        \n",
    "    def stats(self):\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        hit_rate = (self.cache_hits / total) * 100 if total > 0 else 0\n",
    "        return f\"Cache hits: {self.cache_hits}, misses: {self.cache_misses}, hit rate: {hit_rate:.2f}%, size: {len(self.cache)}\"\n",
    "\n",
    "# Global cache instance\n",
    "audio_cache = AudioCache(cache_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction is done once during dataset preparation\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "global_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "\n",
    "# Custom Dataset with optimizations\n",
    "class EmotionAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, preload=False, preprocess_workers=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            preload (bool): Whether to preload all audio files (for small datasets).\n",
    "            preprocess_workers (int): Number of workers for preprocessing (if preload=True).\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.processor = global_processor  # Use the global processor\n",
    "        self.transform = transform\n",
    "        self.invalid_files = set()\n",
    "        self.preload = preload\n",
    "        self.preloaded_data = {}\n",
    "        \n",
    "        # Extract labels from paths\n",
    "        self.labels = []\n",
    "        for path in self.data.iloc[:, 0]:\n",
    "            if 'anger' in path:\n",
    "                self.labels.append(label_to_id['anger'])\n",
    "            elif 'fear' in path:\n",
    "                self.labels.append(label_to_id['fear'])\n",
    "            elif 'happy' in path:\n",
    "                self.labels.append(label_to_id['happy'])\n",
    "            elif 'neutral' in path:\n",
    "                self.labels.append(label_to_id['neutral'])\n",
    "            elif 'sad' in path:\n",
    "                self.labels.append(label_to_id['sad'])\n",
    "            else:\n",
    "                self.labels.append(0)  # Default\n",
    "        \n",
    "        # Preload data if specified (for small datasets)\n",
    "        if preload:\n",
    "            print(f\"Preloading dataset with {preprocess_workers} workers...\")\n",
    "            with ThreadPoolExecutor(max_workers=preprocess_workers) as executor:\n",
    "                paths = self.data.iloc[:, 0].tolist()\n",
    "                audio_data = list(tqdm(executor.map(self._load_audio, paths), total=len(paths)))\n",
    "                self.preloaded_data = {path: data for path, data in zip(paths, audio_data) if data is not None}\n",
    "            print(f\"Preloaded {len(self.preloaded_data)} valid audio files out of {len(paths)}\")\n",
    "        \n",
    "    def _load_audio(self, audio_path):\n",
    "        try:\n",
    "            # Check cache first\n",
    "            cached_data = audio_cache.get(audio_path)\n",
    "            if cached_data is not None:\n",
    "                return cached_data\n",
    "            \n",
    "            # Load and preprocess audio\n",
    "            import os\n",
    "            if not os.path.exists(audio_path):\n",
    "                return None\n",
    "                \n",
    "            # Use torchaudio for faster loading when possible\n",
    "            try:\n",
    "                waveform, sample_rate = torchaudio.load(audio_path)\n",
    "                \n",
    "                # Convert to mono if stereo\n",
    "                if waveform.shape[0] > 1:\n",
    "                    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "                \n",
    "                # Resample if needed\n",
    "                if sample_rate != TARGET_SAMPLE_RATE:\n",
    "                    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=TARGET_SAMPLE_RATE)\n",
    "                    waveform = resampler(waveform)\n",
    "            except:\n",
    "                # Fallback to librosa\n",
    "                waveform, sample_rate = librosa.load(audio_path, sr=TARGET_SAMPLE_RATE)\n",
    "                waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "            \n",
    "            # Truncate or pad to standard length\n",
    "            if waveform.shape[1] > MAX_AUDIO_LENGTH:\n",
    "                waveform = waveform[:, :MAX_AUDIO_LENGTH]\n",
    "            elif waveform.shape[1] < MAX_AUDIO_LENGTH:\n",
    "                # Zero padding\n",
    "                padding = torch.zeros(1, MAX_AUDIO_LENGTH - waveform.shape[1])\n",
    "                waveform = torch.cat([waveform, padding], dim=1)\n",
    "            \n",
    "            result = waveform.squeeze(0)\n",
    "            # Store in cache\n",
    "            audio_cache.put(audio_path, result)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        try:\n",
    "            audio_path = self.data.iloc[idx, 0]  # Assuming first column is the file path\n",
    "            label_id = self.labels[idx]\n",
    "            \n",
    "            # Get preloaded or cached data if available\n",
    "            if self.preload and audio_path in self.preloaded_data:\n",
    "                waveform = self.preloaded_data[audio_path]\n",
    "            else:\n",
    "                waveform = self._load_audio(audio_path)\n",
    "            \n",
    "            # Handle invalid files\n",
    "            if waveform is None:\n",
    "                if audio_path not in self.invalid_files:\n",
    "                    self.invalid_files.add(audio_path)\n",
    "                # Return a dummy waveform\n",
    "                waveform = torch.zeros(MAX_AUDIO_LENGTH)\n",
    "            \n",
    "            # Apply transform if specified\n",
    "            if self.transform:\n",
    "                waveform = self.transform(waveform)\n",
    "            \n",
    "            return {'waveform': waveform, 'label': label_id, 'path': audio_path}\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Return a dummy sample as fallback\n",
    "            return {'waveform': torch.zeros(MAX_AUDIO_LENGTH), 'label': 0, 'path': 'error_path'}\n",
    "\n",
    "# Batched processing for feature extraction\n",
    "def batch_process_features(waveforms, sampling_rate=16000):\n",
    "    \"\"\"Process a batch of waveforms with the feature extractor\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = global_processor(waveforms, sampling_rate=sampling_rate, padding=True, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized collate function for batching\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Optimized collate function that processes features in batch mode.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get waveforms and labels\n",
    "        waveforms = [item['waveform'].numpy() for item in batch]\n",
    "        labels = torch.tensor([item['label'] for item in batch])\n",
    "        paths = [item['path'] for item in batch]\n",
    "        \n",
    "        # Batch process features\n",
    "        inputs = batch_process_features(waveforms)\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values,\n",
    "            'attention_mask': inputs.attention_mask if hasattr(inputs, 'attention_mask') else None,\n",
    "            'labels': labels,\n",
    "            'paths': paths\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        # Return a minimal batch to prevent crash\n",
    "        dummy_inputs = torch.zeros((len(batch), MAX_AUDIO_LENGTH))\n",
    "        dummy_labels = torch.zeros(len(batch), dtype=torch.long)\n",
    "        return {\n",
    "            'input_values': dummy_inputs,\n",
    "            'attention_mask': None,\n",
    "            'labels': dummy_labels,\n",
    "            'paths': [\"error\"] * len(batch)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized model with mixed precision support\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.3, hidden_size=768):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained HuBERT model with fewer layers for speed\n",
    "        config = AutoConfig.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "        # Reduce number of encoder layers for faster training\n",
    "        config.num_hidden_layers = 6  # Half of the original 12 layers\n",
    "        self.hubert_model = HubertModel.from_pretrained(\"facebook/hubert-base-ls960\", config=config)\n",
    "        \n",
    "        # Optional: Freeze some early layers\n",
    "        modules = list(self.hubert_model.encoder.layers.children())[:3]  # Freeze first 3 layers\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Get hidden size from HuBERT\n",
    "        hidden_size = self.hubert_model.config.hidden_size\n",
    "        \n",
    "        # Lightweight attention pooling instead of transformer encoder\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        # Extract features with HuBERT\n",
    "        outputs = self.hubert_model(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get the hidden states\n",
    "        hidden_states = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        attention_weights = self.attention(hidden_states)  # [batch_size, sequence_length, 1]\n",
    "        context_vector = torch.sum(hidden_states * attention_weights, dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(context_vector)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and optimization\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3, \n",
    "                grad_accum_steps=2, mixed_precision=True, scheduler_type='cosine'):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = 'best_emotion_model.pth'\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Mixed precision setup\n",
    "    scaler = torch.cuda.amp.GradScaler() if mixed_precision and torch.cuda.is_available() else None\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "    elif scheduler_type == 'warmup':\n",
    "        from transformers import get_cosine_schedule_with_warmup\n",
    "        num_training_steps = len(train_loader) // grad_accum_steps * num_epochs\n",
    "        num_warmup_steps = int(0.1 * num_training_steps)  # 10% of total steps for warmup\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, \n",
    "                                                   num_training_steps=num_training_steps)\n",
    "    \n",
    "    print(f\"Starting training with mixed precision: {mixed_precision}, gradient accumulation steps: {grad_accum_steps}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        valid_batches = 0\n",
    "        optimizer.zero_grad()  # Zero gradients at start of epoch\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for i, batch in enumerate(pbar):\n",
    "            try:\n",
    "                input_values = batch['input_values'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device) if batch['attention_mask'] is not None else None\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Mixed precision forward pass\n",
    "                if mixed_precision and torch.cuda.is_available():\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(input_values, attention_mask)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        # Scale loss by gradient accumulation steps\n",
    "                        loss = loss / grad_accum_steps\n",
    "                    \n",
    "                    # Scale gradients and backprop\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step every grad_accum_steps or at the end of an epoch\n",
    "                    if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "                        # Unscale before clipping\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        # Gradient clipping\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        # Optimizer step with scaler\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        # Update LR scheduler if using warmup\n",
    "                        if scheduler_type == 'warmup':\n",
    "                            scheduler.step()\n",
    "                else:\n",
    "                    # Standard precision training\n",
    "                    outputs = model(input_values, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / grad_accum_steps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Step every grad_accum_steps or at the end of an epoch\n",
    "                    if (i + 1) % grad_accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "                        # Gradient clipping\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        # Update LR scheduler if using warmup\n",
    "                        if scheduler_type == 'warmup':\n",
    "                            scheduler.step()\n",
    "                \n",
    "                # Track statistics (use the unscaled loss)\n",
    "                running_loss += loss.item() * grad_accum_steps\n",
    "                valid_batches += 1\n",
    "                \n",
    "                # Get predictions for accuracy calculation\n",
    "                with torch.no_grad():\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': loss.item() * grad_accum_steps,\n",
    "                    'batch_acc': (predicted == labels).sum().item() / len(labels)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if valid_batches == 0:\n",
    "            print(\"No valid batches in epoch, skipping\")\n",
    "            continue\n",
    "            \n",
    "        epoch_loss = running_loss / valid_batches\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds) if len(all_preds) > 0 else 0\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "        \n",
    "        # Print cache stats\n",
    "        print(audio_cache.stats())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        valid_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                try:\n",
    "                    input_values = batch['input_values'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device) if batch['attention_mask'] is not None else None\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    # Mixed precision validation (optional)\n",
    "                    if mixed_precision and torch.cuda.is_available():\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(input_values, attention_mask)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                    else:\n",
    "                        outputs = model(input_values, attention_mask)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    valid_val_batches += 1\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    all_val_preds.extend(predicted.cpu().numpy())\n",
    "                    all_val_labels.extend(labels.cpu().numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if valid_val_batches == 0:\n",
    "            print(\"No valid validation batches, skipping validation\")\n",
    "            continue\n",
    "            \n",
    "        val_epoch_loss = val_loss / valid_val_batches\n",
    "        val_epoch_acc = accuracy_score(all_val_labels, all_val_preds) if len(all_val_preds) > 0 else 0\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}\")\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler.step()\n",
    "        elif scheduler_type == 'plateau':\n",
    "            scheduler.step(val_epoch_loss)\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"Model saved to {best_model_path}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Plot training and validation metrics\n",
    "    if len(train_losses) > 0 and len(val_losses) > 0:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title('Loss over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.title('Accuracy over epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.show()\n",
    "    \n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with optimizations\n",
    "def evaluate_model(model, test_loader, criterion, mixed_precision=True):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device) if batch['attention_mask'] is not None else None\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Mixed precision evaluation (optional)\n",
    "            if mixed_precision and torch.cuda.is_available():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_values, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(input_values, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=emotion_labels, \n",
    "                yticklabels=emotion_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_loss, accuracy, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function with optimizations\n",
    "def main():  \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.backends.cudnn.benchmark = True  # Enable cuDNN auto-tuner\n",
    "    \n",
    "    # Enable deterministic algorithms for reproducibility if needed\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    # Data path\n",
    "    csv_file = \"../labeled_data_copy.csv\"\n",
    "    \n",
    "    # RTX 4060 has 8GB VRAM, optimize batch size and model size accordingly\n",
    "    batch_size = 16  # Increased batch size\n",
    "    mixed_precision = True  # Enable mixed precision training\n",
    "    grad_accum_steps = 4  # Accumulate gradients for larger effective batch size\n",
    "    num_workers = 4  # Parallel data loading\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = EmotionAudioDataset(csv_file, preload=False)  # Set preload=True for small datasets\n",
    "    \n",
    "    # Split dataset\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    print(f\"Test set size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,  # Drop last batch to avoid issues with batch norm\n",
    "        persistent_workers=True  # Keep workers alive between epochs\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model with optimizations\n",
    "    model = EmotionClassifier(num_classes=len(emotion_labels)).to(device)\n",
    "    \n",
    "    # Print model summary and parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = optim.AdamW(\n",
    "        [\n",
    "            {'params': model.hubert_model.parameters(), 'lr': 1e-5},  # Lower LR for pretrained\n",
    "            {'params': model.attention.parameters(), 'lr': 3e-4},\n",
    "            {'params': model.classifier.parameters(), 'lr': 3e-4}\n",
    "        ],\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Loss function with label smoothing for regularization\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Train model with optimizations\n",
    "    best_model_path = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimizer,\n",
    "        num_epochs=10,\n",
    "        patience=3,\n",
    "        grad_accum_steps=grad_accum_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "        scheduler_type='warmup'  # Options: 'cosine', 'plateau', 'warmup'\n",
    "    )\n",
    "    \n",
    "    # Clear memory before evaluation\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_f1, cm = evaluate_model(model, test_loader, criterion, mixed_precision=mixed_precision)\n",
    "    \n",
    "    print(\"Training and evaluation complete!\")\n",
    "    print(f\"Best model saved to: {best_model_path}\")\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1 score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Save final model with metadata\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_mapping': id_to_label,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'model_config': {\n",
    "            'hidden_size': model.hubert_model.config.hidden_size,\n",
    "            'num_layers': model.hubert_model.config.num_hidden_layers,\n",
    "            'num_classes': len(emotion_labels)\n",
    "        }\n",
    "    }, 'emotion_classifier_final.pth')\n",
    "    \n",
    "    print(\"Final model saved to: emotion_classifier_final.pth\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
