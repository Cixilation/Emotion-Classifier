{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b3124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "class Wav2VecAudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"file_path\"]\n",
    "        label = int(row[\"label\"])\n",
    "\n",
    "        speech_array, _ = torchaudio.load(path)\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "\n",
    "        inputs = self.processor(speech_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values.squeeze(0)\n",
    "        attention_mask = inputs.attention_mask.squeeze(0) if \"attention_mask\" in inputs else None\n",
    "\n",
    "        return input_values, attention_mask, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38832dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MS24-1\\Environments\\environments\\speech_recognition\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MS24-1\\Environments\\environments\\speech_recognition\\lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "train_dataset = Wav2VecAudioDataset(train_df, processor)\n",
    "val_dataset = Wav2VecAudioDataset(val_df, processor)\n",
    "test_dataset = Wav2VecAudioDataset(test_df, processor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_values = [item[0] for item in batch]\n",
    "    attention_masks = [item[1] for item in batch]\n",
    "    labels = torch.tensor([item[2] for item in batch])\n",
    "\n",
    "    input_values_padded = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True)\n",
    "\n",
    "    if any(mask is not None for mask in attention_masks):\n",
    "        attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True)\n",
    "    else:\n",
    "        attention_masks_padded = None\n",
    "\n",
    "    return input_values_padded, attention_masks_padded, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "472aa514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.weight', 'project_q.bias', 'project_hid.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Wav2VecClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Wav2VecClassifier, self).__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=768, num_heads=4, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.wav2vec(input_values=input_values, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [B, T, 768]\n",
    "\n",
    "        attn_output, _ = self.attention(hidden_states, hidden_states, hidden_states)  # [B, T, 768]\n",
    "        pooled = attn_output.mean(dim=1)  # [B, 768]\n",
    "\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for input_values, attention_mask, labels in dataloader:\n",
    "        input_values = input_values.to(device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_values, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total * 100\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_values, attention_mask, labels in dataloader:\n",
    "            input_values = input_values.to(device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_values, attention_mask)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total * 100\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Wav2VecClassifier(num_classes=7).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, device)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model_wav2vec_attention.pth\")\n",
    "        print(\"Saved new best model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"best_model_wav2vec_attention.pth\"))\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
