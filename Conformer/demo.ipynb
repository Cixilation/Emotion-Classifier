{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.3):\n",
    "        super(ConformerBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv block\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        # Self-attention block\n",
    "        residual = x\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward block\n",
    "        residual = x\n",
    "        ff_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "class ConformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_blocks=8, hidden_dim=384, dropout=0.3):\n",
    "        super(ConformerModel, self).__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.conformer_blocks = nn.ModuleList([\n",
    "            ConformerBlock(hidden_dim, dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.global_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (B, T, input_dim)\n",
    "        x = self.input_proj(x)\n",
    "        for block in self.conformer_blocks:\n",
    "            x = block(x)\n",
    "        x = self.global_norm(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConformerModel(\n",
       "  (input_proj): Linear(in_features=512, out_features=384, bias=True)\n",
       "  (conformer_blocks): ModuleList(\n",
       "    (0-7): 8 x ConformerBlock(\n",
       "      (conv1): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (global_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=384, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 512\n",
    "num_classes = 5 \n",
    "\n",
    "# Inisialisasi dan load model\n",
    "model = ConformerModel(input_dim=input_dim, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(\"best_model_Conformer_Crema.pth\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, sample_rate=16000, n_mels=512):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(waveform)\n",
    "\n",
    "    waveform = waveform.mean(dim=0)  \n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=n_mels)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalisasi\n",
    "    mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-6)\n",
    "\n",
    "    return torch.tensor(mel_spec_db).unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion Class: 0\n",
      "Predicted Emotion: Anger\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"03-01-05-01-02-02-18.wav\"  \n",
    "audio_tensor = preprocess_audio(audio_path) \n",
    "emotion_label = predict_emotion(audio_tensor)\n",
    "\n",
    "print(f\"Predicted Emotion Class: {emotion_label}\")\n",
    "\n",
    "label_map = {0: \"Anger\", 1: \"Fear\", 2: \"Happy\", 3: \"Neutral\", 4: \"Sad\"}  \n",
    "print(f\"Predicted Emotion: {label_map.get(emotion_label, 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion Class: 1\n",
      "Predicted Emotion: Fear\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"03-01-01-01-01-01-04.wav\"  \n",
    "audio_tensor = preprocess_audio(audio_path) \n",
    "emotion_label = predict_emotion(audio_tensor)\n",
    "\n",
    "print(f\"Predicted Emotion Class: {emotion_label}\")\n",
    "\n",
    "label_map = {0: \"Anger\", 1: \"Fear\", 2: \"Happy\", 3: \"Neutral\", 4: \"Sad\"}  \n",
    "print(f\"Predicted Emotion: {label_map.get(emotion_label, 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
