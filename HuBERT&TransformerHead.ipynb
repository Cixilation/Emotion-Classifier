{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc1e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MS24-1\\Environments\\environments\\speech_recognition\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MS24-1\\Environments\\environments\\speech_recognition\\lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import HubertModel, Wav2Vec2Processor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "class HubertTransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes,\n",
    "                 hubert_model=\"facebook/hubert-base-ls960\",\n",
    "                 n_heads=8, n_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(hubert_model)\n",
    "        self.hubert = HubertModel.from_pretrained(hubert_model)\n",
    "        embed_dim = self.hubert.config.hidden_size\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        outputs = self.hubert(input_values=input_values, attention_mask=attention_mask)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        encoded = self.transformer_encoder(hidden)\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "\n",
    "class Wav2VecAudioDataset(Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"file_path\"]\n",
    "        label = int(row[\"label\"])\n",
    "        waveform, _ = torchaudio.load(path)\n",
    "        waveform = waveform.squeeze().numpy()\n",
    "        return waveform, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    input_values = self.feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    attention_mask = inputs.attention_mask\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return input_values, attention_mask, labels\n",
    "\n",
    "# Load and split dataset\n",
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42)\n",
    "\n",
    "# Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "# Datasets and Loaders\n",
    "train_dataset = Wav2VecAudioDataset(train_df, processor)\n",
    "val_dataset = Wav2VecAudioDataset(val_df, processor)\n",
    "test_dataset = Wav2VecAudioDataset(test_df, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HubertTransformerClassifier(num_classes=7).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train and evaluation functions\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    for wave, mask, labels in loader:\n",
    "        wave, mask, labels = wave.to(device), mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(wave, mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return loss_sum / len(loader), correct / total * 100\n",
    "\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for wave, mask, labels in loader:\n",
    "            wave, mask, labels = wave.to(device), mask.to(device), labels.to(device)\n",
    "            preds = model(wave, mask).argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496406e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(10):\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader)\n",
    "    val_acc = eval_epoch(model, val_loader)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} ▶ Loss {tr_loss:.4f} | Train Acc {tr_acc:.2f}% | Val Acc {val_acc:.2f}%\")\n",
    "\n",
    "torch.save(best_model_state, \"best_hubert_transformer_model.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_hubert_transformer_model.pt\"))\n",
    "test_acc = eval_epoch(model, test_loader)\n",
    "print(f\"✅ Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
